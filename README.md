# What is this?

Docker image based on https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source

## Docker Image

### Build

```bash
docker build -f DockerfileAbstratiumPython39 -t abstratium/python39-bitnet .
```

### Run

```bash
docker run \
    --rm -it \
    --name abstratium-bitnet-llm \
    -p 127.0.0.1:19000:8080 \
    abstratium/python39-bitnet
```

### Access

```bash
http://127.0.0.1:19000/
```


## Settings in the UI

### Completions

Completes the prompt with the next word(s)

### Chat

#### ğŸ”¢ Predictions

Usually refers to the number of tokens to generate.

Can also mean the number of completions to return (in some systems).

Example: If set to 100, the model will try to generate up to 100 tokens in response.

#### ğŸŒ¡ï¸ Temperature

Controls creativity/randomness of the output.

Higher = more random, lower = more focused/predictable.


Temperature	Behavior Example
0.2	Very deterministic (safe, boring)
0.7	Balanced creativity
1.0+	Wild, unexpected outputs

#### ğŸ” Penalize repeat sequence

Discourages the model from repeating the same sequences of tokens.

Helps reduce annoying repetition in generated text.

#### ğŸ”¢ Consider N tokens for penalize
This tells the model to look back at the last N tokens to see if anything is being repeated.

Example: If set to 50, the model avoids repeating phrases from the last 50 tokens.

â†©ï¸ Penalize repetition of newlines
Specifically avoids repeating \n or blank lines.

Useful when generating lists, code, or markdown, to avoid endless line breaks.

#### ğŸ¯ Top-K sampling

Limits the model to picking from only the top K most likely tokens.

If K = 50, it samples from the top 50 most likely next words, not all possible ones.

A small K = safer and more focused output. Big K = more diversity.

#### ğŸ² Top-P sampling (a.k.a. nucleus sampling)

Instead of a fixed number like Top-K, this picks from the top tokens whose total probability adds up to P.

If P = 0.9, it selects from the smallest set of tokens that together have a 90% chance of being correct.

More adaptive than Top-K â€” good balance of quality + diversity.

#### ğŸ“‰ Min-P sampling

Filters out all tokens below a certain probability threshold.

Example: If Min-P = 0.01, any token with a probability lower than 1% is ignored.

Helps avoid weird or low-likelihood tokens creeping in.

#### ğŸ§  TL;DR â€” Recommended for most tasks:

| Setting | Recommended Starting Point |
|---|---|
| Temperature | 0.7 |
| Top-K | 40â€“100 |
| Top-P | 0.8â€“0.95 |
| Min-P | 0.01 |
| Penalize Repeats | On (true) |
| Repeat window (N) | 50â€“100 |


#### ğŸ“ Prompt

This is the initial instruction or context sent to the model before the conversation.

Often includes persona, tone, or task guidance.

Example:

"You are a helpful customer support agent. Be concise and polite."

This sets the tone for how the model behaves in the entire session.

#### ğŸ‘¤ User Name

Just the display name or label used for the person talking to the bot.

It doesnâ€™t affect the model behavior unless you use it in the prompt template or chat history template.

#### ğŸ¤– Bot Name

Same as above, but for the model.

Again, doesn't affect model behavior directly unless you're inserting it into structured prompts.

#### ğŸ§© Prompt Template

This defines how the system builds the final prompt that's sent to the model for each message.

You can include placeholders like:

```
{{user}}, {{bot}}

{{history}}

{{message}}
```

Example template:

```
{{history}}
{{user}}: {{message}}
{{bot}}:
```

This template controls how the chat gets serialized into a format the model can understand. Super important for chat-based models.

#### ğŸ•°ï¸ Chat History Template

Similar to the prompt template, but controls how previous messages are formatted.

Used to build {{history}} in the prompt template.

Example:

```
{{user}}: {{message}}
{{bot}}: {{response}}
```

If you want more formatting like role tags or markdown, youâ€™d customize it here.

#### ğŸ“š Grammar

Often refers to structured output formatting (not grammar as in English rules).

Some LLaMA servers (like OpenRouter or tools like LMQL) support structured JSON output via "grammar-based decoding".

If enabled, it forces the model to generate only valid outputs per a defined grammar â€” e.g., force it to return JSON, XML, or a form.

Example use:

Require output like:

```json
{ "label": "spam" }
```

The grammar would define that structure, and the model is constrained to follow it.

#### TL;DR:

| Setting	|Purpose|
|---|---|
| Prompt	|Initial instructions / persona setup|
| User name / Bot name	|Cosmetic, or used in templating|
| Prompt template	|Formats each prompt to send to the model|
| Chat history template	|Controls how prior turns are shown|
| Grammar	|Optional strict format for model outputs (e.g., JSON)|